{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.misc\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "import scipy.misc\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read dataset1\n",
    "full_face = []\n",
    "full_labels = []\n",
    "all_images = os.listdir('yalefaces')\n",
    "for j in range(1,16):    # there are 15 different people in the dataset\n",
    "    img_list = [filename for filename in all_images if filename.startswith('subject%.2d'%j)]\n",
    "#     img_list = glob.glob('yalefaces/subject%.2d*'%j)\n",
    "    face1 = np.zeros((len(img_list),64,64))\n",
    "    for k,img in enumerate(img_list):\n",
    "        I = scipy.misc.imread(os.path.join('yalefaces',img))\n",
    "        I = cv2.fastNlMeansDenoising(I, None, 9, 13)\n",
    "        I = cv2.GaussianBlur(I, (5,5), 0)\n",
    "        I = cv2.equalizeHist(I)\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            I,\n",
    "            scaleFactor=1.1,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30))\n",
    "        try : \n",
    "            x,y,h,w = faces[0]\n",
    "            face1[k,...] = zoom(I[y:y+h,x:x+w],[64./h,64./w])     \n",
    "        except : \n",
    "            x,y,h,w=[0,0,479,639]\n",
    "            face1[k,...] = zoom(I[y:y+h,x:x+w],[64./h,64./w])   \n",
    "        \n",
    "    labels = (j-1)*np.ones((11,))\n",
    "    full_face.append(face1)\n",
    "    full_labels.append(labels)\n",
    "full_face = np.stack(full_face)    # contains the images of all faces\n",
    "full_labels = np.stack(full_labels)   # contains the ID of all the faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 11, 64, 64)\n",
      "(15, 11)\n"
     ]
    }
   ],
   "source": [
    "full_face = full_face/255 #normalise data\n",
    "print(full_face.shape)\n",
    "print(full_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similar 1 dissimilar 0\n",
    "# 2 images of same and different people\n",
    "similarity=[]\n",
    "labels = []\n",
    "pairs = []\n",
    "sim=0\n",
    "nonsim=0\n",
    "for i in range(0,40000):\n",
    "        b = np.random.randint(0,15,2)\n",
    "        a = np.random.randint(0,11,2)\n",
    "        \n",
    "        \n",
    "        if(b[0]==b[1]):\n",
    "            similarity.append(1)\n",
    "            labels.append(b)\n",
    "            pairs.append(a)\n",
    "            sim+=1\n",
    "        else:\n",
    "            if nonsim>sim : \n",
    "                continue\n",
    "            else : \n",
    "                similarity.append(0)\n",
    "                labels.append(b)\n",
    "                pairs.append(a)\n",
    "                nonsim+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207 2603\n"
     ]
    }
   ],
   "source": [
    "print(len(similarity),sum(similarity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = np.zeros((len(similarity),64,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity,euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((len(similarity),64*64))\n",
    "for i in range(len(similarity)):\n",
    "    a = labels[i]\n",
    "    b = pairs[i]\n",
    "    face1 = full_face[a[0],b[0],:,:]\n",
    "    face2 = full_face[a[1],b[1],:,:]\n",
    "    face = face1-face2\n",
    "    face= np.multiply(face,face)#abs(np.multiply(face1,face1)-np.multiply(face2,face2))#cosine_similarity(face1,face2)\n",
    "    #face=cosine_similarity(face3,face3)\n",
    "    trainX[i,:,:]=face\n",
    "    X_train[i,:]=trainX[i,:,:].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.88788927e-01, 2.51964629e-01, 3.41422530e-01, ...,\n",
       "        5.17339485e-02, 8.19530950e-02, 2.88642830e-01],\n",
       "       [5.61030373e-01, 5.72841215e-01, 2.51964629e-01, ...,\n",
       "        3.01422530e-03, 3.11418685e-02, 7.97231834e-02],\n",
       "       [4.44444444e-01, 3.98631296e-01, 1.53787005e-01, ...,\n",
       "        7.44329104e-03, 3.84467512e-04, 2.58515955e-02],\n",
       "       ...,\n",
       "       [1.13740869e-01, 1.63152634e-01, 2.48043060e-01, ...,\n",
       "        1.33010381e-01, 1.33010381e-01, 9.59784698e-02],\n",
       "       [1.03406382e-01, 1.50726644e-01, 1.86082276e-01, ...,\n",
       "        1.11111111e-01, 1.24567474e-01, 1.41730104e-01],\n",
       "       [2.10519031e-01, 2.48043060e-01, 1.96370627e-01, ...,\n",
       "        1.33010381e-01, 1.05943868e-01, 9.35640138e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5207, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1,xtest1,ytrain,ytest=train_test_split(X_train, similarity,test_size=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3384, 4096)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# if PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(whiten=True)\n",
    "pca.fit(xtrain1)\n",
    "n_components = np.argmin(np.cumsum(pca.explained_variance_ratio_)<=0.7)\n",
    "print(n_components)\n",
    "xtrain=pca.transform(xtrain1)\n",
    "xtrain=xtrain[:,:n_components]\n",
    "xtest=pca.transform(xtest1)\n",
    "xtest=xtest[:,:n_components]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2009, 1) (670, 1)\n"
     ]
    }
   ],
   "source": [
    "# if LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "lda=LinearDiscriminantAnalysis()\n",
    "lda.fit(xtrain1,ytrain)\n",
    "xtrain=lda.transform(xtrain1)\n",
    "xtest=lda.transform(xtest1)\n",
    "print(xtrain.shape,xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if nothing\n",
    "xtrain=xtrain1\n",
    "xtest=xtest1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.9731087470449172\n",
      "Test Accuracy :  0.9391113549094898\n",
      "Train F1 Score :  0.9733684518583552\n",
      "Test F1 Score :  0.9419152276295132\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf=SVC(C=15.0,kernel='rbf')\n",
    "clf.fit(xtrain,ytrain)\n",
    "print('Train Accuracy : ',clf.score(xtrain,ytrain))\n",
    "print('Test Accuracy : ',clf.score(xtest,ytest))\n",
    "ypred_train=clf.predict(xtrain)\n",
    "ypred_test=clf.predict(xtest)\n",
    "print('Train F1 Score : ',f1_score(y_pred=ypred_train,y_true=ytrain))\n",
    "print('Test F1 Score : ',f1_score(y_pred=ypred_test,y_true=ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.9612884160756501\n",
      "Test Accuracy :  0.90016456390565\n",
      "Train F1 Score :  0.9623671358804942\n",
      "Test F1 Score :  0.9067622950819672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf=DecisionTreeClassifier(criterion='entropy',max_depth=12)#,max_features='sqrt')\n",
    "clf.fit(xtrain,ytrain)\n",
    "print('Train Accuracy : ',clf.score(xtrain,ytrain))\n",
    "print('Test Accuracy : ',clf.score(xtest,ytest))\n",
    "ypred_train=clf.predict(xtrain)\n",
    "ypred_test=clf.predict(xtest)\n",
    "print('Train F1 Score : ',f1_score(y_pred=ypred_train,y_true=ytrain))\n",
    "print('Test F1 Score : ',f1_score(y_pred=ypred_test,y_true=ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(2000, 1000, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
       "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(2000,1000,10), random_state=1)\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(1), random_state=1)\n",
    "clf.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.9857377948436643\n",
      "Train F1 Score :  1.0\n",
      "Test F1 Score :  0.986021505376344\n"
     ]
    }
   ],
   "source": [
    "print('Train Accuracy : ',clf.score(xtrain,ytrain))\n",
    "print('Test Accuracy : ',clf.score(xtest,ytest))\n",
    "ypred_train=clf.predict(xtrain)\n",
    "ypred_test=clf.predict(xtest)\n",
    "print('Train F1 Score : ',f1_score(y_pred=ypred_train,y_true=ytrain))\n",
    "print('Test F1 Score : ',f1_score(y_pred=ypred_test,y_true=ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.946051602814699\n",
      "Train F1 Score :  1.0\n",
      "Test F1 Score :  0.9469638739431206\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf=LogisticRegression(C=50)\n",
    "clf.fit(xtrain,ytrain)\n",
    "print('Train Accuracy : ',clf.score(xtrain,ytrain))\n",
    "print('Test Accuracy : ',clf.score(xtest,ytest))\n",
    "ypred_train=clf.predict(xtrain)\n",
    "ypred_test=clf.predict(xtest)\n",
    "print('Train F1 Score : ',f1_score(y_pred=ypred_train,y_true=ytrain))\n",
    "print('Test F1 Score : ',f1_score(y_pred=ypred_test,y_true=ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  0.941193853427896\n",
      "Test Accuracy :  0.9040043883708173\n",
      "Train F1 Score :  0.9418638621092609\n",
      "Test F1 Score :  0.9065670048051255\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=7,random_state=7)\n",
    "clf.fit(xtrain,ytrain)\n",
    "print('Train Accuracy : ',clf.score(xtrain,ytrain))\n",
    "print('Test Accuracy : ',clf.score(xtest,ytest))\n",
    "ypred_train=clf.predict(xtrain)\n",
    "ypred_test=clf.predict(xtest)\n",
    "print('Train F1 Score : ',f1_score(y_pred=ypred_train,y_true=ytrain))\n",
    "print('Test F1 Score : ',f1_score(y_pred=ypred_test,y_true=ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy :  1.0\n",
      "Test Accuracy :  0.9687328579264948\n",
      "Train F1 Score :  1.0\n",
      "Test F1 Score :  0.9692722371967655\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf=BaggingClassifier(base_estimator=DecisionTreeClassifier(criterion='entropy',max_depth=40),n_estimators=100)\n",
    "clf.fit(xtrain,ytrain)\n",
    "print('Train Accuracy : ',clf.score(xtrain,ytrain))\n",
    "print('Test Accuracy : ',clf.score(xtest,ytest))\n",
    "ypred_train=clf.predict(xtrain)\n",
    "ypred_test=clf.predict(xtest)\n",
    "print('Train F1 Score : ',f1_score(y_pred=ypred_train,y_true=ytrain))\n",
    "print('Test F1 Score : ',f1_score(y_pred=ypred_test,y_true=ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('test.h5','r')\n",
    "a = list(h5f.keys())\n",
    "## read the dataset\n",
    "full_face = []\n",
    "full_labels = []\n",
    "img_list=[h5f[i] for i in a]\n",
    "tface1 = np.zeros((len(img_list),64,64))\n",
    "for k,img in enumerate(img_list):\n",
    "    I = np.array(img)#scipy.misc.imread(os.path.join('yalefaces',img))\n",
    "    #print(type(I))\n",
    "    I = cv2.fastNlMeansDenoising(I, None, 9, 13)\n",
    "    I = cv2.GaussianBlur(I, (5,5), 0)\n",
    "    I = cv2.equalizeHist(I)\n",
    "    faces = faceCascade.detectMultiScale(\n",
    "        I,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30))\n",
    "    #print(k)\n",
    "    #print(faces)\n",
    "    try : \n",
    "        x,y,h,w = faces[0]\n",
    "        tface1[k,...] = zoom(I[y:y+h,x:x+w],[64./h,64./w])\n",
    "    except : \n",
    "        x,y,h,w=[0,0,111,91]\n",
    "        tface1[k,...] = zoom(I[y:y+h,x:x+w],[64./h,64./w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tface1 = tface1/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pairs = np.genfromtxt('image_pairs.csv', dtype=None, delimiter=',', names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(2000, 1000, 10), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120, 64, 64)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(clf)\n",
    "tface1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n"
     ]
    }
   ],
   "source": [
    "euc=[]\n",
    "\n",
    "for i in range(3540):#(3540):\n",
    "    x1 = np.zeros((64,64))\n",
    "    x0 = np.zeros((64,64))\n",
    "    ids = test_pairs[i][0]\n",
    "    image1=test_pairs[i][1]\n",
    "    image2=test_pairs[i][2]\n",
    "    x0 = tface1[a.index(str(test_pairs[i][1])),:,:]\n",
    "    x1 = tface1[a.index(str(test_pairs[i][2])),:,:]\n",
    "    if(i%1000==0):\n",
    "        print(i)\n",
    "    face = x0-x1\n",
    "    face = np.multiply(face,face)#abs(np.multiply(face1,face1)-np.multiply(face2,face2))#cosine_similarity(face1,face2)\n",
    "    #face=cosine_similarity(face3,face3)\n",
    "    face=face.ravel()\n",
    "    inp = np.zeros((1,64*64))\n",
    "    inp[0,:]=face\n",
    "    #inp=pca.transform(inp)\n",
    "    #inp=inp[:,:n_components]\n",
    "    euc.append(clf.predict(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([231])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 is similar; 0 is different\n",
    "with open('to_submit/2mlp.csv','w') as f : \n",
    "    f.write('Id,Expected\\n')\n",
    "    for i in range(len(test_pairs)) : \n",
    "        ids = test_pairs[i][0]\n",
    "        temp = euc[i][0]\n",
    "        f.write(str(ids)+','+str(temp)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euc = np.asarray(euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1367"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
